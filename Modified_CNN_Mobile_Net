# -*- coding: utf-8 -*-
if not os.path.exists('/content/brain_tumor_dataset'):
    print("Dataset not found. Downloading...")
    from google.colab import files
    if not os.path.exists('/root/.kaggle/kaggle.json'):
        print("Please upload your kaggle.json file:")
        files.upload()
        !mkdir -p ~/.kaggle
        !mv kaggle.json ~/.kaggle/
        !chmod 600 ~/.kaggle/kaggle.json
    !kaggle datasets download -d navoneel/brain-mri-images-for-brain-tumor-detection
    !unzip -q brain-mri-images-for-brain-tumor-detection.zip
    print("Dataset downloaded and extracted!")
else:
    print("Dataset already exists. Skipping download.")

import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
import cv2
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, WeightedRandomSampler
import torchmetrics
import copy
from PIL import Image

no_tumor = "/content/brain_tumor_dataset/no"
yes_tumor = "/content/brain_tumor_dataset/yes"

if os.path.exists("/content/brain_tumor_dataset/train"):
    print("Split dataset already exists. Skipping data preparation...")
else:
    print("Preparing and splitting dataset...")
    no_tumor_images = [os.path.join(no_tumor, img) for img in os.listdir(no_tumor)]
    yes_tumor_images = [os.path.join(yes_tumor, img) for img in os.listdir(yes_tumor)]
    no_tumor_label = [0] * len(no_tumor_images)
    yes_tumor_label = [1] * len(yes_tumor_images)
    total_images = no_tumor_images + yes_tumor_images
    total_labels = no_tumor_label + yes_tumor_label
    X_train, X_temp, y_train, y_temp = train_test_split(total_images, total_labels, test_size=0.3, stratify=total_labels, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)
    output_dir = {
        "train": "/content/brain_tumor_dataset/train",
        "val": "/content/brain_tumor_dataset/val",
        "test": "/content/brain_tumor_dataset/test"
    }
    for split in output_dir:
        os.makedirs(os.path.join(output_dir[split], "yes_tumor"), exist_ok=True)
        os.makedirs(os.path.join(output_dir[split], "no_tumor"), exist_ok=True)
    def copy_images(image_paths, labels, split):
        for img_path, label in zip(image_paths, labels):
            category = "yes_tumor" if label == 1 else "no_tumor"
            dest_path = os.path.join(output_dir[split], category, os.path.basename(img_path))
            shutil.copy(img_path, dest_path)
    copy_images(X_train, y_train, "train")
    copy_images(X_val, y_val, "val")
    copy_images(X_test, y_test, "test")
    print(f"Training: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}")
    print("Dataset split completed!")

img_size = (224, 224)
batch_size = 16

train_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomCrop(img_size),
    transforms.RandomRotation(20),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.3),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

eval_transform = transforms.Compose([
    transforms.Resize(img_size),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_dataset = datasets.ImageFolder("/content/brain_tumor_dataset/train", transform=train_transform)
val_dataset = datasets.ImageFolder("/content/brain_tumor_dataset/val", transform=eval_transform)
test_dataset = datasets.ImageFolder("/content/brain_tumor_dataset/test", transform=eval_transform)

class_counts = np.bincount(train_dataset.targets)
class_weights = 1. / class_counts
sample_weights = [class_weights[label] for label in train_dataset.targets]
sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)

train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)

print(f"Class distribution - No tumor: {class_counts[0]}, Yes tumor: {class_counts[1]}")

class EnhancedMobileNet(nn.Module):
    def __init__(self, num_classes=2, dropout=0.5):
        super(EnhancedMobileNet, self).__init__()
        self.mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        in_features = self.mobilenet.classifier[1].in_features
        self.mobilenet.classifier = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(in_features, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout/2),
            nn.Linear(512, num_classes)
        )
    def forward(self, x):
        return self.mobilenet(x)

class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0
    def check_early_stop(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, patience=7):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    early_stopping = EarlyStopping(patience=patience)
    best_val_acc = 0.0
    best_model_state = None
    accuracy_metric = torchmetrics.classification.Accuracy(task="binary").to(device)
    f1_metric = torchmetrics.classification.F1Score(task="binary").to(device)
    recall_metric = torchmetrics.classification.Recall(task="binary").to(device)
    precision_metric = torchmetrics.classification.Precision(task="binary").to(device)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        accuracy_metric.reset()
        f1_metric.reset()
        recall_metric.reset()
        precision_metric.reset()
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            accuracy_metric.update(preds, labels)
            f1_metric.update(preds, labels)
            recall_metric.update(preds, labels)
            precision_metric.update(preds, labels)
        train_loss /= len(train_loader)
        train_acc = accuracy_metric.compute().item()
        train_f1 = f1_metric.compute().item()
        train_recall = recall_metric.compute().item()
        train_precision = precision_metric.compute().item()
        model.eval()
        val_loss = 0.0
        accuracy_metric.reset()
        f1_metric.reset()
        recall_metric.reset()
        precision_metric.reset()
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                preds = torch.argmax(outputs, dim=1)
                accuracy_metric.update(preds, labels)
                f1_metric.update(preds, labels)
                recall_metric.update(preds, labels)
                precision_metric.update(preds, labels)
        val_loss /= len(val_loader)
        val_acc = accuracy_metric.compute().item()
        val_f1 = f1_metric.compute().item()
        val_recall = recall_metric.compute().item()
        val_precision = precision_metric.compute().item()
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        print(f"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}")
        scheduler.step()
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = copy.deepcopy(model.state_dict())
            torch.save(best_model_state, "best_model.pth")
            print(f"  âœ“ Model saved! Best Val Acc: {best_val_acc:.4f}")
        if early_stopping.check_early_stop(val_loss):
            print(f"\nEarly stopping triggered after {epoch + 1} epochs!")
            break
    model.load_state_dict(best_model_state)
    print(f"\n{'='*50}")
    print(f"Training completed! Best validation accuracy: {best_val_acc:.4f}")
    print(f"{'='*50}")
    return model

model = EnhancedMobileNet(num_classes=2, dropout=0.5)
class_weights_tensor = torch.FloatTensor([class_weights[0], class_weights[1]])
device = "cuda" if torch.cuda.is_available() else "cpu"
class_weights_tensor = class_weights_tensor.to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)
trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, patience=7)

def predict_with_tta(model, image_path, num_augmentations=10):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.eval()
    tta_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image = Image.open(image_path).convert("RGB")
    predictions = []
    with torch.no_grad():
        for _ in range(num_augmentations):
            input_tensor = tta_transform(image).unsqueeze(0).to(device)
            output = model(input_tensor)
            probs = F.softmax(output, dim=1)
            predictions.append(probs.cpu().numpy())
    avg_prediction = np.mean(predictions, axis=0)
    predicted_class = np.argmax(avg_prediction)
    confidence = avg_prediction[0, predicted_class]
    class_labels = ["no_tumor", "yes_tumor"]
    predicted_label = class_labels[predicted_class]
    print(f"Predicted: {predicted_label} (Confidence: {confidence:.4f})")
    return predicted_class, predicted_label, confidence

def evaluate_model(model, test_loader):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.eval()
    accuracy_metric = torchmetrics.classification.Accuracy(task="binary").to(device)
    f1_metric = torchmetrics.classification.F1Score(task="binary").to(device)
    recall_metric = torchmetrics.classification.Recall(task="binary").to(device)
    precision_metric = torchmetrics.classification.Precision(task="binary").to(device)
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = torch.argmax(outputs, dim=1)
            accuracy_metric.update(preds, labels)
            f1_metric.update(preds, labels)
            recall_metric.update(preds, labels)
            precision_metric.update(preds, labels)
    test_acc = accuracy_metric.compute().item()
    test_f1 = f1_metric.compute().item()
    test_recall = recall_metric.compute().item()
    test_precision = precision_metric.compute().item()
    print(f"\n{'='*50}")
    print(f"TEST SET RESULTS:")
    print(f"{'='*50}")
    print(f"Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"F1 Score:  {test_f1:.4f}")
    print(f"Recall:    {test_recall:.4f}")
    print(f"Precision: {test_precision:.4f}")
    print(f"{'='*50}\n")
    return test_acc, test_f1, test_recall, test_precision

evaluate_model(trained_model, test_loader)
